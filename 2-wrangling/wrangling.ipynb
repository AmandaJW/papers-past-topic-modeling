{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Data Wrangling\n",
    "---\n",
    "### Papers Past Topic Modeling\n",
    "\n",
    "<br/>\n",
    "\n",
    "Ben Faulks - bmf43@uclive.ac.nz\n",
    "\n",
    "Xiandong Cai - xca24@uclive.ac.nz\n",
    "\n",
    "Yujie Cui - ycu23@uclive.ac.nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.id', 'local-1547688430970'),\n",
      " ('spark.app.name', 'local'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.driver.host', '192.168.1.207'),\n",
      " ('spark.driver.memory', '62g'),\n",
      " ('spark.master', 'local[*]'),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.driver.port', '35011'),\n",
      " ('spark.ui.showConsoleProgress', 'true'),\n",
      " ('spark.driver.cores', '6'),\n",
      " ('spark.driver.maxResultSize', '4g')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.207:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=local>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "sys.path.insert(0, '../utils') # for import customed modules\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from utils_data import conf_pyspark, load_dataset\n",
    "\n",
    "# intiate PySpark\n",
    "sc, spark = conf_pyspark()\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load raw dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (16731578, 6)\n",
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|      id|                 url|         publisher|                time|               title|             content|\n",
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|15266415|http://api.digita...|Poverty Bay Herald|1912-09-11T00:00:...|FOOTBALL. (Povert...|FOOTBALL.Advice w...|\n",
      "|16064096|http://api.digita...|Poverty Bay Herald|1914-01-30T00:00:...|SCHOLARSHIPS. (Po...|SCHOLARSHIPS.EDUC...|\n",
      "|16555594|http://api.digita...|Poverty Bay Herald|1916-12-21T00:00:...|Page 10 Advertise...|GOMMON, SHELTON 1...|\n",
      "| 7969531|http://api.digita...|Poverty Bay Herald|1891-06-13T00:00:...|AUSTRALIAN. (Pove...|AUSTRALIAN.— , « ...|\n",
      "| 9359478|http://api.digita...|Poverty Bay Herald|1895-03-21T00:00:...|Page 3 Advertisem...|pOM.%OK«- CHELTON...|\n",
      "| 9977629|http://api.digita...|Poverty Bay Herald|1897-10-20T00:00:...|HOME-BAKING A PLE...|HOME-BAKING A PLE...|\n",
      "| 2967335|http://api.digita...| Manawatu Standard|1884-08-05T00:00:...|COMMERCIAL NEWS. ...|COMMERCIAL NEWS.W...|\n",
      "|34118995|http://api.digita...|    Mataura Ensign|1908-07-03T12:00:00Z|ACCIDENT TO MAJOR...|ACCIDENT TO MAJOR...|\n",
      "| 3061647|http://api.digita...|      Bruce Herald|1881-10-07T00:00:...|THE LAND BOARD. (...|b\"THE LAND BOARD....|\n",
      "| 5961764|http://api.digita...|          NZ Truth|1923-08-18T00:00:...|INOLIS  (Hayne's ...|INOLIS (Hayne's H...|\n",
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_dataset('papers_past', spark)\n",
    "\n",
    "nrow_raw = df.count()\n",
    "print('Shape of dataframe: ({}, {})'.format(nrow_raw, len(df.columns)))\n",
    "df.sample(False, 0.00001).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check empty values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Null:\n",
      "+---+---+---------+----+-----+-------+\n",
      "| id|url|publisher|time|title|content|\n",
      "+---+---+---------+----+-----+-------+\n",
      "|  0|  0|        0|   0|    0|  56232|\n",
      "+---+---+---------+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count null\n",
    "print('Print Null:')\n",
    "df.select([F.count(F.when(F.col(c).isNull() | (F.col(c) == ''), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop rows with empty document:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean NA to avoid nonetype.\n",
    "df = df.na.drop(subset=['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check again:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Null:\n",
      "+---+---+---------+----+-----+-------+\n",
      "| id|url|publisher|time|title|content|\n",
      "+---+---+---------+----+-----+-------+\n",
      "|  0|  0|        0|   0|    0|      0|\n",
      "+---+---+---------+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count null\n",
    "print('Print Null:')\n",
    "df.select([F.count(F.when(F.col(c).isNull() | (F.col(c) == ''), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Duplicate Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The \"id\" should be unique, check duplication:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated id number:  543700\n"
     ]
    }
   ],
   "source": [
    "print('Duplicated id number: ', df.count() - df.select('id').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are duplicated \"id\" in the dataset, show three of them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|      id|count|\n",
      "+--------+-----+\n",
      "|10036037|    2|\n",
      "|10059447|    2|\n",
      "|10099968|    2|\n",
      "+--------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('id').count().where(F.col('count')>1).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the first one to check detail:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|      id|                 url|         publisher|                time|               title|             content|\n",
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|10036037|http://api.digita...|Poverty Bay Herald|1898-01-06T00:00:...|THE EASTERN SITUA...|THE EASTERN SITUA...|\n",
      "|10036037|http://api.digita...|Poverty Bay Herald|1898-01-06T00:00:...|THE EASTERN SITUA...|THE EASTERN SITUA...|\n",
      "+--------+--------------------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.id == 10036037).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check difference of the content:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:  0.9994846688997681\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "str1 = df.filter(df.id == 10036037).select('content').collect()[0]['content']\n",
    "#print(str1 + '\\n')\n",
    "\n",
    "str2 = df.filter(df.id == 10036037).select('content').collect()[1]['content']\n",
    "#print(str2 + '\\n')\n",
    "\n",
    "diff = difflib.SequenceMatcher(None, str1, str2).ratio()\n",
    "\n",
    "print('Similarity: ', diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The two duplicates are very close, drop one of them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check duplicate again:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated id number:  0\n"
     ]
    }
   ],
   "source": [
    "print('Duplicated id number: ', df.count() - df.select('id').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Abnormal Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There should be 68 publishers, check numbers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "Correct! no abnormal values in publishers.\n"
     ]
    }
   ],
   "source": [
    "n = df.select('publisher').distinct().count()\n",
    "print(n)\n",
    "if n == 68:\n",
    "    print('Correct! no abnormal values in publishers.')\n",
    "else:\n",
    "    print('Error! abnormal values in publishers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For history documents, it only need date as time unit, we extract \"date\" column from \"time\" column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature date\n",
    "df = df.withColumn('date', df['time'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check schema of the dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check date range has abnormal values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.date(1839, 8, 21), datetime.date(1945, 12, 31))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = df.select(F.min('date'), F.max('date')).first()\n",
    "start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Advertisements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check \"title\" column to see if it is possible to extract features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------------------------------------------------------------+\n",
      "|id      |date      |title                                                                 |\n",
      "+--------+----------+----------------------------------------------------------------------+\n",
      "|8842388 |1883-07-31|CABLE NEWS (Star, 31 July 1883)                                       |\n",
      "|18580019|1917-08-21|SICK AND WOUNDED (Evening Post, 21 August 1917)                       |\n",
      "|24781165|1911-10-20|THE MARY ISABEL. (Bay Of Plenty Times, 20 October 1911)               |\n",
      "|29742577|1918-07-27|ENGAGEMENTS. (Wairarapa Daily Times, 27 July 1918)                    |\n",
      "|24623498|1921-09-22|SPORTING (Hawera & Normanby Star, 22 September 1921)                  |\n",
      "|23872577|1886-04-20|THE NEW CROMWELL COMPANY'S MINE. (Otago Daily Times, 20 April 1886)   |\n",
      "|26690600|1917-06-26|Page 8 Advertisements Column 1 (Colonist, 26 June 1917)               |\n",
      "|2974035 |1888-08-25|Be Sure that You Need It. (Tuapeka Times, 25 August 1888)             |\n",
      "|19692995|1932-01-05|LATE SHIPPING (Evening Post, 05 January 1932)                         |\n",
      "|27228937|1932-09-27|Page 4 Advertisements Column 3 (Ellesmere Guardian, 27 September 1932)|\n",
      "+--------+----------+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(False, 0.00001).limit(10).select('id', 'date', 'title').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The \"title\" column specified advertisement, we extract \"ads\" column from \"title\" column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------------------------------------------------------------------------------------------+\n",
      "|id      |ads  |title                                                                                             |\n",
      "+--------+-----+--------------------------------------------------------------------------------------------------+\n",
      "|18956558|false|ROBBERY ALLEGED (Evening Post, 08 August 1938)                                                    |\n",
      "|23795844|true |Page 4 Advertisements Column 3 (Feilding Star, 25 September 1920)                                 |\n",
      "|2101377 |false|LATEST TELEGRAMS. (Grey River Argus, 06 December 1866)                                            |\n",
      "|3261438 |false|THE  Grey River Argus.  PUBLISHED DAILY  THURSDAY, JUNE 25, 1874. (Grey River Argus, 25 June 1874)|\n",
      "|7111402 |true |Page 3 Advertisements Column 4 (Ashburton Guardian, 03 February 1899)                             |\n",
      "|6563253 |false|AN UNFOUNDED RUMOUR. (Marlborough Express, 25 August 1887)                                        |\n",
      "|16358883|false|DEATHS. (Star, 05 December 1908)                                                                  |\n",
      "|16556738|true |Page 1 Advertisements Column 5 (Thames Star, 14 June 1918)                                        |\n",
      "|9126431 |false|AUSTRALIA. (Southland Times, 08 June 1886)                                                        |\n",
      "|19542805|false|POST OFFICE NOTICE. MAILS CLOSE (Evening Post, 29 April 1872)                                     |\n",
      "+--------+-----+--------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract feature ads\n",
    "df = df.withColumn('ads', df.title.contains('dvertisement'))\n",
    "\n",
    "df.sample(False, 0.00001).limit(10).select('id', 'ads', 'title').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The title consists of three parts: \"real title\" (\"publisher\", \"date\"), we only need \"real title\" part. Extract real title:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redandunt parts of title\n",
    "df = df.withColumn('title_', F.regexp_extract(F.col('title'), '(.*)(\\s\\(.*\\))', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if some titles are not the form \"title (\"publisher\", \"date\"), which will lead to \"title_\" column is empty string:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------------------+\n",
      "|id      |title_|title                |\n",
      "+--------+------+---------------------+\n",
      "|3656781 |      |Untitled Illustration|\n",
      "|4832017 |      |Untitled Illustration|\n",
      "|5417742 |      |Untitled Illustration|\n",
      "|12676570|      |Untitled Illustration|\n",
      "|12777321|      |Untitled Illustration|\n",
      "+--------+------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(F.col('title_') == '').select(['id', 'title_', 'title']).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change empty string in \"title_\" column to \"Untitled Illustration\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    'title_',\n",
    "    F.when(\n",
    "        F.col('title_') == '',\n",
    "        F.lit('Untitled Illustration')\n",
    "    ).otherwise(\n",
    "        F.col('title_')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check empty string again:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Null:\n",
      "+---+---+---------+----+-----+-------+----+---+------+\n",
      "| id|url|publisher|time|title|content|date|ads|title_|\n",
      "+---+---+---------+----+-----+-------+----+---+------+\n",
      "|  0|  0|        0|   0|    0|      0|   0|  0|     0|\n",
      "+---+---+---------+----+-----+-------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count null\n",
    "print('Print Null:')\n",
    "df.select([F.count(F.when(F.col(c).isNull() | (F.col(c) == ''), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print title columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------------------------------------+------------------------------------------------------------------------------------------------+\n",
      "|id      |title_                                                         |title                                                                                           |\n",
      "+--------+---------------------------------------------------------------+------------------------------------------------------------------------------------------------+\n",
      "|4370517 |MR DILLON BELL IN THE THEATEE.                                 |MR DILLON BELL IN THE THEATEE. (Southland Times, 23 April 1872)                                 |\n",
      "|9051094 |A. Railway Collision.                                          |A. Railway Collision. (Wanganui Herald, 07 September 1889)                                      |\n",
      "|2237617 |SALVATION ARMY.                                                |SALVATION ARMY. (Hawera & Normanby Star, 17 April 1883)                                         |\n",
      "|7997099 |DIED OF HIS INJURIES.                                          |DIED OF HIS INJURIES. (Grey River Argus, 12 January 1885)                                       |\n",
      "|15214027|THRILLING  HALF-HOUR  SKETCHES.                                |THRILLING  HALF-HOUR  SKETCHES. (Star, 19 August 1902)                                          |\n",
      "|4710328 |Page 4 Advertisements Column 3                                 |Page 4 Advertisements Column 3 (Colonist, 29 December 1868)                                     |\n",
      "|27117231|MURDER MYSTERY                                                 |MURDER MYSTERY (Northern Advocate, 26 August 1920)                                              |\n",
      "|19328836|Evening Post. WEDNESDAY, NOVEMBER 27, 1929. MAKING IT POLITICAL|Evening Post. WEDNESDAY, NOVEMBER 27, 1929. MAKING IT POLITICAL (Evening Post, 27 November 1929)|\n",
      "|17921832|SUBURBAN DEVELOPMENT                                           |SUBURBAN DEVELOPMENT (Evening Post, 13 June 1928)                                               |\n",
      "|2693943 |THE MANAWATU PURCHASE.                                         |THE MANAWATU PURCHASE. (North Otago Times, 21 December 1866)                                    |\n",
      "+--------+---------------------------------------------------------------+------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(False, 0.00001).limit(10).select('id', 'title_', 'title').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the website of [Papers Past](https://paperspast.natlib.govt.nz), we could find the publisher-region relationship in the [Explore all newspapers](https://paperspast.natlib.govt.nz/newspapers/all#region) webpage. Based on this webpage, we could extract region feature from \"publisher\" column. Here we saved [the webpage](https://paperspast.natlib.govt.nz/newspapers/all#region) and crawling the publisher-region relationship into a dataframe for extract feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# read webpage\n",
    "path = r'../temp/Papers Past _ Explore all newspapers.html'\n",
    "with open(path, 'r') as f:\n",
    "    html = f.read()\n",
    "\n",
    "# get table \n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "table = soup.find('table', attrs={'class':'table datatable'})\n",
    "table_rows = table.find_all('tr')\n",
    "res = []\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td')\n",
    "    row = [tr.text.strip() for tr in td if tr.text.strip()]\n",
    "    if row:\n",
    "        res.append(row)\n",
    "\n",
    "# transform table to pandas dataframe\n",
    "df_region = pd.DataFrame(res, columns=['publisher_', 'region', 'start_', 'end_']) # column_ means it will be drop later\n",
    "\n",
    "# transform pandas dataframe to pyspark dataframe\n",
    "df_region = spark.createDataFrame(df_region).orderBy('region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (148, 4)\n",
      "+---------------------------------+-----------------+------+----+\n",
      "|publisher_                       |region           |start_|end_|\n",
      "+---------------------------------+-----------------+------+----+\n",
      "|Hot Lakes Chronicle              |Bay of Plenty    |1895  |1910|\n",
      "|Manawatu Standard                |Manawatu-Wanganui|1883  |1923|\n",
      "|Hiiringa i te Whitu              |National         |1896  |1896|\n",
      "|Motueka Star                     |Nelson           |1901  |1938|\n",
      "|Bruce Herald                     |Otago            |1865  |1920|\n",
      "|Dunstan Times                    |Otago            |1866  |1948|\n",
      "|Mount Ida Chronicle              |Otago            |1869  |1926|\n",
      "|Opunake Times                    |Taranaki         |1894  |1949|\n",
      "|King Country Chronicle           |Waikato          |1906  |1920|\n",
      "|Thames Guardian and Mining Record|Waikato          |1871  |1872|\n",
      "+---------------------------------+-----------------+------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataframe: ({}, {})'.format(df_region.count(), len(df_region.columns)))\n",
    "df_region.sample(False, 0.1).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that in this publisher-region relationship dataframe, there are two publisher's name is not identical with the dataset: \"Bay Of Plenty Times\" mismatch by \"of\", \"New Zealand Free Lance\" mismatch by \"New Zeland\", so we modify the** `df_region` **to make it identical with dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+------+----+\n",
      "|publisher_         |region       |start_|end_|\n",
      "+-------------------+-------------+------+----+\n",
      "|Bay of Plenty Times|Bay of Plenty|1872  |1949|\n",
      "|Free Lance         |Wellington   |1900  |1920|\n",
      "+-------------------+-------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_region.filter((df_region.publisher_ == 'Bay of Plenty Times')\n",
    "                 | (df_region.publisher_ == 'Free Lance'))\n",
    " .show(10, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update df_region for Bay Of Plenty Times and New Zealand Free Lance\n",
    "df_region = df_region.withColumn(\n",
    "    'publisher_',\n",
    "    F.when(\n",
    "        F.col('publisher_') == 'Bay of Plenty Times',\n",
    "        F.lit('Bay Of Plenty Times')\n",
    "    ).otherwise(\n",
    "        F.col('publisher_')\n",
    "    )\n",
    ").withColumn(\n",
    "    'publisher_',\n",
    "    F.when(\n",
    "        F.col('publisher_') == 'Free Lance',\n",
    "        F.lit('New Zealand Free Lance')\n",
    "    ).otherwise(\n",
    "        F.col('publisher_')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if the two publishers' name were modified:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+------+----+\n",
      "|publisher_            |region       |start_|end_|\n",
      "+----------------------+-------------+------+----+\n",
      "|Bay Of Plenty Times   |Bay of Plenty|1872  |1949|\n",
      "|New Zealand Free Lance|Wellington   |1900  |1920|\n",
      "+----------------------+-------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_region.filter((df_region.publisher_ == 'Bay Of Plenty Times')\n",
    "                 | (df_region.publisher_ == 'New Zealand Free Lance'))\n",
    " .show(10, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the dataframe for later use:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'../temp/region.csv'\n",
    "\n",
    "(df_region.select(F.col('publisher_'),\n",
    "                  F.col('region'))\n",
    " .toPandas()\n",
    " .to_csv(path, header=False, index=False, encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract region column, and abandon redundant columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.join(df_region, df.publisher == df_region.publisher_, how='left')\n",
    "      .select(F.col('id'), \n",
    "              F.col('publisher'), \n",
    "              F.col('region'), \n",
    "              F.col('date'), \n",
    "              F.col('ads'), \n",
    "              F.col('title_').alias('title'), \n",
    "              F.col('content'))\n",
    "      .orderBy('id')\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputing missing value in region column with \"unknwon\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill({'region':'unknown'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if miss any field or element:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Null and empty string:\n",
      "+---+---------+------+----+---+-----+-------+\n",
      "| id|publisher|region|date|ads|title|content|\n",
      "+---+---------+------+----+---+-----+-------+\n",
      "|  0|        0|     0|   0|  0|    0|      0|\n",
      "+---+---------+------+----+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Print Null and empty string:')\n",
    "df.select([F.count(F.when(F.col(c).isNull() | (F.col(c) == ''), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check dataframe szie:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (16131646, 7)\n",
      "usable line percentage: 0.9641437287026962\n",
      "removed line number: 599932\n"
     ]
    }
   ],
   "source": [
    "nrow = df.count()\n",
    "print('Shape of dataframe: ({}, {})'.format(nrow, len(df.columns)))\n",
    "print('usable line percentage:', nrow/nrow_raw)\n",
    "print('removed line number:', nrow_raw - nrow)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After data wrangling, there are:**\n",
    "* 16,131,646 (96.4%) samples/rows/lines/documents usable, \n",
    "* 599,932 samples/rows/lines/documents were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print schema and dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- region: string (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      " |-- ads: boolean (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n",
      "+-------+--------------------+-----------------+----------+-----+--------------------+--------------------+\n",
      "|     id|           publisher|           region|      date|  ads|               title|             content|\n",
      "+-------+--------------------+-----------------+----------+-----+--------------------+--------------------+\n",
      "|2008528|       Bush Advocate|      Hawke's Bay|1891-01-10| true|Page 1 Advertisem...|Business Notices....|\n",
      "|2117223|  Ellesmere Guardian|       Canterbury|1897-07-10| true|Page 4 Advertisem...|« I?XTI:.AOEDTN\\R...|\n",
      "|2248386|    Grey River Argus|       West Coast|1869-09-25|false|        HAWKE'S BAY.|HAWKE'S BAY.The f...|\n",
      "|2997494|            NZ Truth|         National|1909-01-02|false|STEP-FATHER AND S...|STEP-FATHER AND S...|\n",
      "|3039133|            Colonist|           Nelson|1884-08-28|false|SHIPPING INTELLIG...|SHIPPING INTELLIG...|\n",
      "|3161730|       Otago Witness|            Otago|1856-11-08| true|Page 2 Advertisem...|SALE OF TOWN LAND...|\n",
      "|3514291|     Manawatu Herald|Manawatu-Wanganui|1893-04-29|false|Sir John Hall's S...|Sir John Hall's S...|\n",
      "|3586248|   Manawatu Standard|Manawatu-Wanganui|1885-03-21|false|     CORRESPONDENCE.|CORRESPONDENCE.(T...|\n",
      "|3731528|       Timaru Herald|       Canterbury|1871-01-25|false|            Untitled|Maravilla Cocoa. ...|\n",
      "|3936549|            Observer|         Auckland|1889-08-10|false|Is the Earth Glob...|\"Is the Earth Glo...|\n",
      "|3959350|Otautau Standard ...|            Otago|1916-04-18| true|Page 1 Advertisem...|-f FIRM FSINpAf I...|\n",
      "|3973547|Daily Southern Cross|         Auckland|1864-05-06|false|         POST OFFICE|POST O FFICEMuis:...|\n",
      "|4237500|       Timaru Herald|       Canterbury|1873-05-14| true|Page 4 Advertisem...|. Extract from pa...|\n",
      "|4349792|  Ashburton Guardian|       Canterbury|1893-03-29|false|   School Committee.|School Committee....|\n",
      "|4408082|     Southland Times|            Otago|1873-02-25| true|Page 1 Advertisem...|Bnsiness Notice*....|\n",
      "|4477867|     Taranaki Herald|         Taranaki|1870-12-10| true|Page 1 Advertisem...|iHcctingg* TARANA...|\n",
      "|4503545|Daily Southern Cross|         Auckland|1865-05-18| true|Page 3 Advertisem...|Public Auotion Sa...|\n",
      "|4579351|  Ashburton Guardian|       Canterbury|1893-12-16| true|Page 1 Advertisem...|Biisipc.3rf iNotl...|\n",
      "|4699194| Marlborough Express|      Marlborough|1884-01-10|false|THE N.Z. INSURANC...|THE N.Z. INSURANC...|\n",
      "|4800804|     Southland Times|            Otago|1875-11-10|false|PORT OF INVERCARG...|PORT OF INVERCARG...|\n",
      "+-------+--------------------+-----------------+----------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.sample(False, 0.00001).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Save Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Dataset for Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This dataframe would be our final dataset to generate metadata and subset to analyze and visualize, let's save it as compressed csv file to save time for later processes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'../data/dataset'\n",
    "df.write.csv(path, mode='overwrite', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the clean dataset size:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw   dataset size: 33G\n",
      "clean dataset size: 14G\n"
     ]
    }
   ],
   "source": [
    "path = r'../data/papers_past'\n",
    "print('raw   dataset size:', subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8'))\n",
    "path = r'../data/dataset'\n",
    "print('clean dataset size:', subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After processing and compressing, the dataset reduce from 33GB to 14GB.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Dataset for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This dataset would be the full dataset for MALLET topic modeling, we will use this trained topic model to infer topic models of subset to analyze and visualize.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(F.col('id'), F.col('title'), F.col('content'))\n",
    "\n",
    "path = r'../data/train'\n",
    "df.write.csv(path, mode='overwrite', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
