{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5.1 - Split Dataset\n",
    "---\n",
    "### Papers Past Topic Modeling\n",
    "<br/>\n",
    "\n",
    "Ben Faulks - bmf43@uclive.ac.nz\n",
    "\n",
    "Xiandong Cai - xca24@uclive.ac.nz\n",
    "\n",
    "Yujie Cui - ycu23@uclive.ac.nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-30 21:30:31\n",
      "[('spark.driver.host', 'x99.hub'),\n",
      " ('spark.app.name', 'local'),\n",
      " ('spark.app.id', 'local-1548837034038'),\n",
      " ('spark.driver.port', '44256'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.driver.memory', '62g'),\n",
      " ('spark.master', 'local[*]'),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.ui.showConsoleProgress', 'true'),\n",
      " ('spark.driver.cores', '6'),\n",
      " ('spark.driver.maxResultSize', '4g')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://x99.hub:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=local>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc, sys, subprocess\n",
    "sys.path.insert(0, '../utils') # for import customed modules\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from utils import conf_pyspark, load_dataset\n",
    "\n",
    "import datetime\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# intiate PySpark\n",
    "sc, spark = conf_pyspark()\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this part, we will build several subsets for topic modeling:**\n",
    "\n",
    "1. build a random sample set from the clean dataset;\n",
    "1. build a training set and meta set from the sample set;\n",
    "1. build subsets from the sample set, based on three typical application scenario: by range of time, by regions, by label.\n",
    "\n",
    "**The data directory tree shows as below:**\n",
    "```\n",
    "project\n",
    "└── data                     # save all data\n",
    "    ├── dataset              # save all processed datasets\n",
    "    │   ├── clean            # save clean dataset\n",
    "    │   └── sample           # save sampled dataset\n",
    "    │       ├── meta         # save metadata of sampled dataset\n",
    "    │       ├── train        # save training dataset of sampled dataset\n",
    "    │       └── subset       # save all subsets from sampled dataset\n",
    "    │           ├── time1    # save subset for a time range\n",
    "    │           ├── time2    # save subset for a time range\n",
    "    │           ├── time...  # save subset for a time range\n",
    "    │           ├── region1  # save subset for a region\n",
    "    │           ├── region2  # save subset for a region\n",
    "    │           ├── region3  # save subset for a region\n",
    "    │           ├── region4  # save subset for a region\n",
    "    │           ├── region...# save subset for a region\n",
    "    │           └── ads      # save subset for ADs\n",
    "    └── papers_past          # save raw dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load clean dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, publisher: string, region: string, date: date, ads: boolean, title: string, content: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('clean', spark)\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (15121970, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataframe: ({}, {})'.format(df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The topic modeling is a computation-intensive task, training the full dataset need powerful computing resource. For the limit of memory and time, we have to downsize the dataset for training. Here we select the strategies for Random Sampling for the aim to cover the most range of documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraint for random sampling\n",
    "PROPORTION = 0.2 # the proportion of sampling\n",
    "SEED = 1          # set seed to reproduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, publisher: string, region: string, date: date, ads: boolean, title: string, content: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df.sample(False, PROPORTION, SEED)\n",
    "df_sample.cache()\n",
    "\n",
    "df.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (3025602, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataframe: ({}, {})'.format(df_sample.count(), len(df_sample.columns)))\n",
    "#df_sample.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataframe** `df_sample` **is the clean sample set, other subset will extract from this dataset. Then we split it to training set and medadata set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, publisher: string, region: string, date: date, ads: boolean]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = (df_sample\n",
    "            .select(F.col('id'), \n",
    "                    F.col('title'), \n",
    "                    F.col('content'))\n",
    "            .orderBy('id'))\n",
    "df_train.cache()\n",
    "\n",
    "df_meta = (df_sample\n",
    "           .select(F.col('id'), \n",
    "                   F.col('publisher'), \n",
    "                   F.col('region'), \n",
    "                   F.col('date'), \n",
    "                   F.col('ads'))\n",
    "           .orderBy('id'))\n",
    "df_meta.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train dataframe: (3025602, 3)\n",
      "Shape of meta  dataframe: (3025602, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train dataframe: ({}, {})'.format(df_train.count(), len(df_train.columns)))\n",
    "print('Shape of meta  dataframe: ({}, {})'.format(df_meta.count(), len(df_meta.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save datasets, and convert compressed files to one .csv file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to ../data/dataset/sample/meta\n",
      "Dataset size: 154M\n"
     ]
    }
   ],
   "source": [
    "path = r'../data/dataset/sample/meta'\n",
    "\n",
    "df_meta.write.csv(path, mode='overwrite')\n",
    "\n",
    "df_meta.unpersist()\n",
    "\n",
    "print('Saved dataset to', path)\n",
    "print('Dataset size:', subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3025602 ../data/dataset/sample/meta/meta.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $path\n",
    "\n",
    "# concatenate multi files to one file\n",
    "cat $1/*.csv > $1/meta.csv\n",
    "\n",
    "rm -f $1/part-0* $1/\\.part-0*\n",
    "\n",
    "# check row number\n",
    "wc -l $1/meta.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to ../data/dataset/sample/train\n",
      "Dataset size: 6.1G\n"
     ]
    }
   ],
   "source": [
    "path = r'../data/dataset/sample/train'\n",
    "\n",
    "df_train.write.csv(path, sep='\\t', mode='overwrite')\n",
    "\n",
    "df_train.unpersist()\n",
    "\n",
    "print('Saved dataset to', path)\n",
    "print('Dataset size:', subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3025602 ../data/dataset/sample/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $path\n",
    "\n",
    "# concatenate multi files to one file\n",
    "cat $1/*.csv > $1/train.csv\n",
    "\n",
    "rm -f $1/part-0* $1/\\.part-0*\n",
    "\n",
    "# check row number\n",
    "wc -l $1/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 By Range of Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For instance, we are interested in the topics in the papers during WWI, so we will research the topic models around the WWI. As wikipedia define it was lasted from 28/7/1914 to 11/11/1918, we expand the time from 1912 to 1921 to analyze and visualize topics during these time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '1912-01-01'\n",
    "END = '1921-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter samples between start and end date, remove advertisements, and generate the subset - wwi:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove advertisements, sampling subset, and select columns.\n",
    "df_sub = (df_sample.filter((df_sample['ads'] == False) & (df_sample['date'] >= START) & (df_sample['date'] <= END)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the date range of the subset is correct:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.date(1921, 12, 31), datetime.date(1912, 1, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_sub.select(F.max(F.col('date')).alias('MAX')).limit(1).collect()[0].MAX, \n",
    " df_sub.select(F.min(F.col('date')).alias('MIN')).limit(1).collect()[0].MIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate subset to infer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, title: string, content: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = df_sub.select(F.col('id'), F.col('title'), F.col('content')).orderBy('id')\n",
    "df_sub.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (567878, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataframe: ({}, {})'.format(df_sub.count(), len(df_sub.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save subset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved subset to ../data/dataset/sample/subset/wwi\n",
      "subset size: 784M\n"
     ]
    }
   ],
   "source": [
    "path = r'../data/dataset/sample/subset/wwi'\n",
    "\n",
    "df_sub.write.csv(path, sep='\\t', mode='overwrite')\n",
    "\n",
    "df_sub.unpersist()\n",
    "\n",
    "print('Saved subset to', path)\n",
    "print('subset size:', subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert compressed files to one .csv file for MALLET:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567878 ../data/dataset/sample/subset/wwi/wwi.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $path\n",
    "\n",
    "# concatenate multi files to one file\n",
    "cat $1/*.csv > $1/wwi.csv\n",
    "\n",
    "rm -f $1/part-0* $1/\\.part-0*\n",
    "\n",
    "# check row number\n",
    "wc -l $1/wwi.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 By Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 16 regions in the full dataset, we focus on the regions that have the most papers (Otago, Canterbury, Manawatu-Wanganui and Wellington), and extract subset for each region seperately.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decide regions to sample:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['Otago', 'Canterbury', 'Manawatu-Wanganui', 'Wellington']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter samples of target regions, remove advertisements, and generate the subset - regions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = []\n",
    "for i in range(len(regions)):\n",
    "    df_sub.append(df_sample.filter(F.col('region').isin(regions[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check region in the subset is correct:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|region|\n",
      "+------+\n",
      "| Otago|\n",
      "+------+\n",
      "\n",
      "+----------+\n",
      "|    region|\n",
      "+----------+\n",
      "|Canterbury|\n",
      "+----------+\n",
      "\n",
      "+-----------------+\n",
      "|           region|\n",
      "+-----------------+\n",
      "|Manawatu-Wanganui|\n",
      "+-----------------+\n",
      "\n",
      "+----------+\n",
      "|    region|\n",
      "+----------+\n",
      "|Wellington|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(regions)):\n",
    "    df_sub[i].select(F.col('region')).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate subset to infer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe 0: (374495, 3)\n",
      "Shape of dataframe 1: (282791, 3)\n",
      "Shape of dataframe 2: (344669, 3)\n",
      "Shape of dataframe 3: (634731, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(regions)):\n",
    "    df_sub[i] = df_sub[i].select(F.col('id'), F.col('title'), F.col('content')).orderBy('id')\n",
    "    df_sub[i].cache()\n",
    "    print('Shape of dataframe {}: ({}, {})'.format(i, df_sub[i].count(), len(df_sub[i].columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save subset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved subset to ../data/dataset/sample/subset/Otago\n",
      "subset size: 1.2G\n",
      "Saved subset to ../data/dataset/sample/subset/Canterbury\n",
      "subset size: 510M\n",
      "Saved subset to ../data/dataset/sample/subset/Manawatu-Wanganui\n",
      "subset size: 605M\n",
      "Saved subset to ../data/dataset/sample/subset/Wellington\n",
      "subset size: 1.3G\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "for i, region in enumerate(regions):\n",
    "    paths.append(r'../data/dataset/sample/subset/'+region)\n",
    "    \n",
    "    df_sub[i].write.csv(paths[i], sep='\\t', mode='overwrite')\n",
    "    \n",
    "    df_sub[i].unpersist()\n",
    "    \n",
    "    print('Saved subset to', paths[i])\n",
    "    print('subset size:', subprocess.check_output(['du','-sh', paths[i]]).split()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert compressed files to one .csv file for MALLET:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = paths[0]\n",
    "region = regions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374495 ../data/dataset/sample/subset/Otago/Otago.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$path\" \"$region\"\n",
    "\n",
    "# concatenate multi files to one file\n",
    "cat $1/*.csv > $1/$2.csv\n",
    "\n",
    "rm -f $1/part-0* $1/\\.part-0*\n",
    "\n",
    "# check row number\n",
    "wc -l $1/$2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = paths[1]\n",
    "region = regions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282791 ../data/dataset/sample/subset/Canterbury/Canterbury.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$path\" \"$region\"\n",
    "\n",
    "# concatenate multi files to one file\n",
    "cat $1/*.csv > $1/$2.csv\n",
    "\n",
    "rm -f $1/part-0* $1/\\.part-0*\n",
    "\n",
    "# check row number\n",
    "wc -l $1/$2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = paths[2]\n",
    "region = regions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344669 ../data/dataset/sample/subset/Manawatu-Wanganui/Manawatu-Wanganui.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$path\" \"$region\"\n",
    "\n",
    "# concatenate multi files to one file\n",
    "cat $1/*.csv > $1/$2.csv\n",
    "\n",
    "rm -f $1/part-0* $1/\\.part-0*\n",
    "\n",
    "# check row number\n",
    "wc -l $1/$2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = paths[3]\n",
    "region = regions[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634731 ../data/dataset/sample/subset/Wellington/Wellington.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$path\" \"$region\"\n",
    "\n",
    "# concatenate multi files to one file\n",
    "cat $1/*.csv > $1/$2.csv\n",
    "\n",
    "rm -f $1/part-0* $1/\\.part-0*\n",
    "\n",
    "# check row number\n",
    "wc -l $1/$2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 By Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is only one label (ads) in the dataset, marks the sample/row/document/text is an advertisemet or not. Advertisements are less information than articles in news paper. However, they are useful to analyze the life of old time. Advertisements take account 27.4% in the full dataset, we extract a subset for advertisements.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter samples of advertisements, and generate the subset - ads:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove advertisements, sampling subset, and select columns.\n",
    "df_sub = df_sample.filter(F.col('ads') == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check labels in the subset are all \"ads\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| ads|\n",
      "+----+\n",
      "|true|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub.select(F.col('ads')).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate subset to infer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, title: string, content: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = df_sub.select(F.col('id'), F.col('title'), F.col('content')).orderBy('id')\n",
    "df_sub.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (841233, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of dataframe: ({}, {})'.format(df_sub.count(), len(df_sub.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save subset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved subset to ../data/dataset/sample/subset/ads\n",
      "subset size: 2.3G\n"
     ]
    }
   ],
   "source": [
    "path = r'../data/dataset/sample/subset/ads'\n",
    "\n",
    "df_sub.write.csv(path, sep='\\t', mode='overwrite')\n",
    "\n",
    "df_sub.unpersist()\n",
    "\n",
    "print('Saved subset to', path)\n",
    "print('subset size:', subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert compressed files to one .csv file for MALLET:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841233 ../data/dataset/sample/subset/ads/ads.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $path\n",
    "\n",
    "# concatenate multi files to one file\n",
    "cat $1/*.csv > $1/ads.csv\n",
    "\n",
    "rm -f $1/part-0* $1/\\.part-0*\n",
    "\n",
    "# check row number\n",
    "wc -l $1/ads.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
