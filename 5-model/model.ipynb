{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Topic Modeling\n",
    "---\n",
    "### Papers Past Topic Modeling\n",
    "<br/>\n",
    "\n",
    "Ben Faulks - bmf43@uclive.ac.nz\n",
    "\n",
    "Xiandong Cai - xca24@uclive.ac.nz\n",
    "\n",
    "Yujie Cui - ycu23@uclive.ac.nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.id', 'local-1547904364416'),\n",
      " ('spark.app.name', 'local'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.driver.host', '192.168.1.207'),\n",
      " ('spark.driver.memory', '62g'),\n",
      " ('spark.master', 'local[*]'),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.driver.port', '38757'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.ui.showConsoleProgress', 'true'),\n",
      " ('spark.driver.cores', '6'),\n",
      " ('spark.driver.maxResultSize', '4g')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.207:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=local>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, subprocess\n",
    "sys.path.insert(0, '../utils') # for import customed modules\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from utils_load import conf_pyspark, load_dataset\n",
    "\n",
    "# intiate PySpark\n",
    "sc, spark = conf_pyspark()\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this part, we will perform following operations:**\n",
    "\n",
    "1. training a topic model using full dataset by MALLET, getting a topic model and topic words;\n",
    "1. splitting several subsets by random, by range of time, by region, and by advertisements;\n",
    "1. inferring subsets from the topic model of full dataset, getting doc-topic matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since MALLET can take one instance per file or one file one instance per line, the only choice for us is one file one instance per line, we need to transform the** `*.csv.gz` **file to one** `.csv` **file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat ../data/train/*.csv.gz > ../data/train/train.csv.gz\n",
    "\n",
    "gunzip ../data/train/train.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check lines/rows/samples/documents of dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160140 ../data/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wc -l ../data/train/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check contents:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1854232</td>\n",
       "      <td>Page 1 Advertisements Column 1</td>\n",
       "      <td>NOTICE.—This Ne?vspaper may b? sent Free by Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1854244</td>\n",
       "      <td>Page 4 Advertisements Column 1</td>\n",
       "      <td>T7JOUND, a set of Pekoe Straps, The JJ owner m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1854262</td>\n",
       "      <td>THE CHRISTIAN CHURCH.</td>\n",
       "      <td>THE CHRISTIAN CHURCH.We have heard of an objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1854275</td>\n",
       "      <td>Page 1 Advertisements Column 2</td>\n",
       "      <td>NOTE PAPER, Bill Paper, Envelopes Memorandum B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1854588</td>\n",
       "      <td>THE EASTERN CRISIS.</td>\n",
       "      <td>THE EASTERN CRISIS.[reuieu's telegrams— copyri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                               1  \\\n",
       "0  1854232  Page 1 Advertisements Column 1   \n",
       "1  1854244  Page 4 Advertisements Column 1   \n",
       "2  1854262           THE CHRISTIAN CHURCH.   \n",
       "3  1854275  Page 1 Advertisements Column 2   \n",
       "4  1854588             THE EASTERN CRISIS.   \n",
       "\n",
       "                                                   2  \n",
       "0  NOTICE.—This Ne?vspaper may b? sent Free by Po...  \n",
       "1  T7JOUND, a set of Pekoe Straps, The JJ owner m...  \n",
       "2  THE CHRISTIAN CHURCH.We have heard of an objec...  \n",
       "3  NOTE PAPER, Bill Paper, Envelopes Memorandum B...  \n",
       "4  THE EASTERN CRISIS.[reuieu's telegrams— copyri...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_table('../data/train/train.csv', header=None, nrows=5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We do not think of the number of topics as a natural characteristic of corpora. The topic number is not really combinations of multinomial distributions, so there is no \"right\" topic number. We think of the number of topics as the scale of a map of corpora. If we want a broad overview, we use a small topic number. If we want more detail, use a larger topic number. The right number is the value that produces meaningful results that allow us to accomplish our goal.**\n",
    "\n",
    "**There is a wide range of good values for us, here we will train the dataset to get a topic model with 500 topics.**\n",
    "\n",
    "**Many metric methods and tools could help us to quantitatively tune the topic number,  such as [ldatuning](https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html) and [topic coherence](https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/), those evaluate work could be our future work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture capt\n",
    "%%time\n",
    "%%bash\n",
    "#! /bin/bash\n",
    "\n",
    "bash ./model.sh -i '../data/train/train.csv' -o './model_train' -p 'train';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write training log to file. This way to avoid MALLET print very long log in notebook.\n",
    "with open('./model_train/train.log', 'w') as f:\n",
    "    f.write(capt.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The output files are:**\n",
    "* topics words from 'topicKeys.txt'\n",
    "* topics distribution per document from 'topicKeys.txt'\n",
    "* topic inferencer for inferring subset from 'inferencer.model'\n",
    "* corpus that topics belong to from 'stat.gz'\n",
    "* statistic info from 'diagnostics.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Except analyze and visualize topic model of full dataset, based on typical application scenario, we could extract several subsets from the full dataset to focus on specific point to analyze.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all, load clean dataset and check dimension:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (160140, 7)\n",
      "+--------+--------------------+-----------------+----------+-----+--------------------+--------------------+\n",
      "|      id|           publisher|           region|      date|  ads|               title|             content|\n",
      "+--------+--------------------+-----------------+----------+-----+--------------------+--------------------+\n",
      "| 2272302|Hawera & Normanby...|         Taranaki|1883-09-26|false|THE EXPLOSIONS AT...|b'THE EXPLOSIONS ...|\n",
      "| 8600214|     Taranaki Herald|         Taranaki|1886-07-01|false|GENERAL ASSEMBLY ...|GENERAL ASSEMBLY ...|\n",
      "|11784635|   Otago Daily Times|            Otago|1882-12-05| true|Page 2 Advertisem...|b'Funeral Notices...|\n",
      "|16100759|  Poverty Bay Herald|         Gisborne|1914-12-29|false|GERMANY'S NICKEL ...|GERMANY'S NICKEL ...|\n",
      "|17702290|        Evening Post|       Wellington|1940-08-28|false|      MAKING OF ARMS|MAKING OF ARMSEXP...|\n",
      "|18027194|        Evening Post|       Wellington|1926-03-29|false|     GOOD CRICKETERS|GOOD CRICKETERSHO...|\n",
      "|18407313|        Evening Post|       Wellington|1910-01-27|false|        COAL STRIKE.|COAL STRIKE.MEETI...|\n",
      "|18946438|        Evening Post|       Wellington|1938-07-18| true|Page 14 Advertise...|BUSINESS NOTICES ...|\n",
      "|24016092|   Otago Daily Times|            Otago|1893-11-22|false|            Untitled|NOMINATIONS. The ...|\n",
      "|25855417|  Wanganui Chronicle|Manawatu-Wanganui|1907-06-15|false|        W.G C.C.G.A.|W.G C.C.G.A.ANNUA...|\n",
      "+--------+--------------------+-----------------+----------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df = load_dataset('dataset', spark)\n",
    "df = load_dataset('dev', spark) # for developement\n",
    "\n",
    "print('Shape of dataframe: ({}, {})'.format(df.count(), len(df.columns)))\n",
    "df.sample(False, 0.0001).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 By Range of Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For instance, we are interested in the topics in the papers during WWI, so we will research the topic models around the WWI. As wikipedia define it was lasted from 28/7/1914 to 11/11/1918, we expand the time from 1912 to 1921 to analyze and visualize topics during these time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decide start date and end date to sample:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '1912-01-01'\n",
    "END = '1921-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter samples between start and end date, remove advertisements, and generate the subset - wwi:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (29789, 7)\n"
     ]
    }
   ],
   "source": [
    "# remove advertisements, sampling subset, and select columns.\n",
    "df_sub = (\n",
    "    df.filter((df['ads'] == False) & (df['date'] >= START) & (df['date'] <= END))\n",
    ")\n",
    "print('Shape of dataframe: ({}, {})'.format(df_sub.count(), len(df_sub.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the date range of the subset is correct:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.date(1921, 12, 31), datetime.date(1912, 1, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_sub.select(F.max(F.col('date')).alias('MAX')).limit(1).collect()[0].MAX, \n",
    " df_sub.select(F.min(F.col('date')).alias('MIN')).limit(1).collect()[0].MIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate subset to infer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (29789, 3)\n"
     ]
    }
   ],
   "source": [
    "df_sub = df_sub.select(F.col('id'), F.col('title'), F.col('content')).orderBy('id')\n",
    "\n",
    "print('Shape of dataframe: ({}, {})'.format(df_sub.count(), len(df_sub.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save subset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save subset to ../data/subset/wwi\n",
      "subset size: 18M\n"
     ]
    }
   ],
   "source": [
    "subset_path = r'../data/subset/wwi'\n",
    "\n",
    "df_sub.write.csv(subset_path, sep='\\t', mode='overwrite', compression='gzip')\n",
    "\n",
    "print('Save subset to', subset_path)\n",
    "print('subset size:', subprocess.check_output(['du','-sh', subset_path]).split()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 By Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 16 regions in the full dataset, we focus on the regions that have the most population now (Auckland, Wellington, Canterbury and Otago).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decide regions to sample:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['Auckland', 'Wellington', 'Canterbury', 'Otago']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter samples of target regions, remove advertisements, and generate the subset - regions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (78072, 7)\n"
     ]
    }
   ],
   "source": [
    "df_sub = df.filter(F.col('region').isin(regions))\n",
    "\n",
    "print('Shape of dataframe: ({}, {})'.format(df_sub.count(), len(df_sub.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check region in the subset is correct:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    region|\n",
      "+----------+\n",
      "|Wellington|\n",
      "|  Auckland|\n",
      "|     Otago|\n",
      "|Canterbury|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub.select(F.col('region')).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate subset to infer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (78072, 3)\n"
     ]
    }
   ],
   "source": [
    "df_sub = df_sub.select(F.col('id'), F.col('title'), F.col('content')).orderBy('id')\n",
    "\n",
    "print('Shape of dataframe: ({}, {})'.format(df_sub.count(), len(df_sub.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save subset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save subset to ../data/subset/regions\n",
      "subset size: 77M\n"
     ]
    }
   ],
   "source": [
    "subset_path = r'../data/subset/regions'\n",
    "\n",
    "df_sub.write.csv(subset_path, sep='\\t', mode='overwrite', compression='gzip')\n",
    "\n",
    "print('Save subset to', subset_path)\n",
    "print('subset size:', subprocess.check_output(['du','-sh', subset_path]).split()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 By Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is only one label (ads) in the dataset, marks the sample/row/document/text is an advertisemet or not. Advertisements are less information than articles in news paper. However, they are useful to analyze the life of old time. Advertisements take account 27.4% in the full dataset, we extract a subset for advertisements.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter samples of advertisements, and generate the subset - ads:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (44175, 7)\n"
     ]
    }
   ],
   "source": [
    "# remove advertisements, sampling subset, and select columns.\n",
    "df_sub = df.filter(F.col('ads') == True)\n",
    "\n",
    "print('Shape of dataframe: ({}, {})'.format(df_sub.count(), len(df_sub.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check labels in the subset are all \"ads\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| ads|\n",
      "+----+\n",
      "|true|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub.select(F.col('ads')).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate subset to infer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (44175, 3)\n"
     ]
    }
   ],
   "source": [
    "df_sub = df_sub.select(F.col('id'), F.col('title'), F.col('content')).orderBy('id')\n",
    "\n",
    "print('Shape of dataframe: ({}, {})'.format(df_sub.count(), len(df_sub.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save subset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save subset to ../data/subset/ads\n",
      "subset size: 55M\n"
     ]
    }
   ],
   "source": [
    "subset_path = r'../data/subset/ads'\n",
    "\n",
    "df_sub.write.csv(subset_path, sep='\\t', mode='overwrite', compression='gzip')\n",
    "\n",
    "print('Save subset to', subset_path)\n",
    "print('subset size:', subprocess.check_output(['du','-sh', subset_path]).split()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Inferring Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We infer subset by inferencer to get doc-topic matrix to analyze and visualize topics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 By Range of Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The same with training full dataset, we transform multiple compressed files to one** `*.csv` **file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat ../data/subset/wwi/*.csv.gz > ../data/subset/wwi/wwi.csv.gz\n",
    "\n",
    "gunzip ../data/subset/wwi/wwi.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check lines/rows/samples/documents of dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29789 ../data/subset/wwi/wwi.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wc -l ../data/subset/wwi/wwi.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check contents:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3025974</td>\n",
       "      <td>Confirmations at Te Aute.</td>\n",
       "      <td>Confirmations at Te Aute.During the last quart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3034188</td>\n",
       "      <td>Diocesan Notes.</td>\n",
       "      <td>Diocesan Notes.A recent letter received from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3042724</td>\n",
       "      <td>Untitled</td>\n",
       "      <td>We all want quiet ; we all want beauty for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3045343</td>\n",
       "      <td>Diocesan Paper.</td>\n",
       "      <td>Diocesan Paper.The following sums are acknow- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3050177</td>\n",
       "      <td>Rotorua.</td>\n",
       "      <td>Rotorua.Vicar: Yen. Archdeacon Tisdall, M.A. C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                          1  \\\n",
       "0  3025974  Confirmations at Te Aute.   \n",
       "1  3034188            Diocesan Notes.   \n",
       "2  3042724                   Untitled   \n",
       "3  3045343            Diocesan Paper.   \n",
       "4  3050177                   Rotorua.   \n",
       "\n",
       "                                                   2  \n",
       "0  Confirmations at Te Aute.During the last quart...  \n",
       "1  Diocesan Notes.A recent letter received from t...  \n",
       "2  We all want quiet ; we all want beauty for the...  \n",
       "3  Diocesan Paper.The following sums are acknow- ...  \n",
       "4  Rotorua.Vicar: Yen. Archdeacon Tisdall, M.A. C...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_table('../data/subset/wwi/wwi.csv', header=None, nrows=5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inferring:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputFile=../data/subset/wwi/wwi.csv\n",
      "OutputDir=./model_wwi\n",
      "Process=infer\n",
      "AllDir=./model_train\n",
      "Inferencer=./model_train/inferencer.model\n",
      "CORES=6\n",
      "SEED1=1\n",
      "SEED2=1\n",
      "TOPICS=250\n",
      "ITERATION=3000\n",
      "INTERVAL=40\n",
      "BURNIN=300\n",
      "IDFMIN=1\n",
      "IDFMAX=10\n",
      "04:04:12 :: Start import dataset...\n",
      " Rewriting extended pipe from ./model_train/import.model\n",
      "  Instance ID = 73e48a25-531d-4bd9-8b7e-acbfdaced195\n",
      "Import new data for inferring.\n",
      "04:04:41 :: Imported.\n",
      "04:04:41 :: Start prune model...\n",
      "04:04:53 :: Pruned.\n",
      "04:04:53 :: Start infering dataset...\n",
      "04:05:34 :: Inferred.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training portion = 1.0\n",
      "Validation portion = 0.0\n",
      "Testing portion = 0.0\n",
      "Prune info gain = 0\n",
      "Prune count = 0\n",
      "Prune df = 0\n",
      "idf range = 1.0-10.0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "features: 1414645 -> 261501\n",
      "Writing instance list to ./model_wwi/pruned.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 8 ms, total: 16 ms\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%capture capt\n",
    "%%time\n",
    "%%bash\n",
    "#! /bin/bash\n",
    "\n",
    "bash ./model.sh -i '../data/subset/wwi/wwi.csv' -o './model_wwi' -p 'infer';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write training log to file. This way to avoid MALLET print very long log in notebook.\n",
    "with open('./model_wwi/wwi.log', 'w') as f:\n",
    "    f.write(capt.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 By Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfor dataset files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat ../data/subset/regions/*.csv.gz > ../data/subset/regions/regions.csv.gz\n",
    "\n",
    "gunzip ../data/subset/regions/regions.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check lines/rows/samples/documents of dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78072 ../data/subset/regions/regions.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wc -l ../data/subset/regions/regions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check contents:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1854232</td>\n",
       "      <td>Page 1 Advertisements Column 1</td>\n",
       "      <td>NOTICE.—This Ne?vspaper may b? sent Free by Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1854244</td>\n",
       "      <td>Page 4 Advertisements Column 1</td>\n",
       "      <td>T7JOUND, a set of Pekoe Straps, The JJ owner m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1854262</td>\n",
       "      <td>THE CHRISTIAN CHURCH.</td>\n",
       "      <td>THE CHRISTIAN CHURCH.We have heard of an objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1854275</td>\n",
       "      <td>Page 1 Advertisements Column 2</td>\n",
       "      <td>NOTE PAPER, Bill Paper, Envelopes Memorandum B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1854588</td>\n",
       "      <td>THE EASTERN CRISIS.</td>\n",
       "      <td>THE EASTERN CRISIS.[reuieu's telegrams— copyri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                               1  \\\n",
       "0  1854232  Page 1 Advertisements Column 1   \n",
       "1  1854244  Page 4 Advertisements Column 1   \n",
       "2  1854262           THE CHRISTIAN CHURCH.   \n",
       "3  1854275  Page 1 Advertisements Column 2   \n",
       "4  1854588             THE EASTERN CRISIS.   \n",
       "\n",
       "                                                   2  \n",
       "0  NOTICE.—This Ne?vspaper may b? sent Free by Po...  \n",
       "1  T7JOUND, a set of Pekoe Straps, The JJ owner m...  \n",
       "2  THE CHRISTIAN CHURCH.We have heard of an objec...  \n",
       "3  NOTE PAPER, Bill Paper, Envelopes Memorandum B...  \n",
       "4  THE EASTERN CRISIS.[reuieu's telegrams— copyri...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_table('../data/subset/regions/regions.csv', header=None, nrows=5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inferring:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputFile=../data/subset/regions/regions.csv\n",
      "OutputDir=./model_regions\n",
      "Process=infer\n",
      "AllDir=./model_train\n",
      "Inferencer=./model_train/inferencer.model\n",
      "CORES=6\n",
      "SEED1=1\n",
      "SEED2=1\n",
      "TOPICS=250\n",
      "ITERATION=3000\n",
      "INTERVAL=40\n",
      "BURNIN=300\n",
      "IDFMIN=1\n",
      "IDFMAX=10\n",
      "04:05:35 :: Start import dataset...\n",
      " Rewriting extended pipe from ./model_train/import.model\n",
      "  Instance ID = 73e48a25-531d-4bd9-8b7e-acbfdaced195\n",
      "Import new data for inferring.\n",
      "04:06:19 :: Imported.\n",
      "04:06:19 :: Start prune model...\n",
      "04:06:35 :: Pruned.\n",
      "04:06:35 :: Start infering dataset...\n",
      "04:09:21 :: Inferred.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training portion = 1.0\n",
      "Validation portion = 0.0\n",
      "Testing portion = 0.0\n",
      "Prune info gain = 0\n",
      "Prune count = 0\n",
      "Prune df = 0\n",
      "idf range = 1.0-10.0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "features: 1414645 -> 130373\n",
      "Writing instance list to ./model_regions/pruned.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 8 ms, total: 32 ms\n",
      "Wall time: 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%capture capt\n",
    "%%time\n",
    "%%bash\n",
    "#! /bin/bash\n",
    "\n",
    "bash ./model.sh -i '../data/subset/regions/regions.csv' -o './model_regions' -p 'infer';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write training log to file. This way to avoid MALLET print very long log in notebook.\n",
    "with open('./model_regions/regions.log', 'w') as f:\n",
    "    f.write(capt.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 By Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfor dataset files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat ../data/subset/ads/*.csv.gz > ../data/subset/ads/ads.csv.gz\n",
    "\n",
    "gunzip ../data/subset/ads/ads.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check lines/rows/samples/documents of dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44175 ../data/subset/ads/ads.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wc -l ../data/subset/ads/ads.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check contents:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1854232</td>\n",
       "      <td>Page 1 Advertisements Column 1</td>\n",
       "      <td>NOTICE.—This Ne?vspaper may b? sent Free by Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1854244</td>\n",
       "      <td>Page 4 Advertisements Column 1</td>\n",
       "      <td>T7JOUND, a set of Pekoe Straps, The JJ owner m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1854275</td>\n",
       "      <td>Page 1 Advertisements Column 2</td>\n",
       "      <td>NOTE PAPER, Bill Paper, Envelopes Memorandum B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1855273</td>\n",
       "      <td>Page 3 Advertisements Column 7</td>\n",
       "      <td>Business Notices. '-; NOTICE. DANEVIREE SASH, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1855701</td>\n",
       "      <td>Page 4 Advertisements Column 7</td>\n",
       "      <td>NEEDHAM'S POLISHING PASTE. Used by Her Majesty...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                               1  \\\n",
       "0  1854232  Page 1 Advertisements Column 1   \n",
       "1  1854244  Page 4 Advertisements Column 1   \n",
       "2  1854275  Page 1 Advertisements Column 2   \n",
       "3  1855273  Page 3 Advertisements Column 7   \n",
       "4  1855701  Page 4 Advertisements Column 7   \n",
       "\n",
       "                                                   2  \n",
       "0  NOTICE.—This Ne?vspaper may b? sent Free by Po...  \n",
       "1  T7JOUND, a set of Pekoe Straps, The JJ owner m...  \n",
       "2  NOTE PAPER, Bill Paper, Envelopes Memorandum B...  \n",
       "3  Business Notices. '-; NOTICE. DANEVIREE SASH, ...  \n",
       "4  NEEDHAM'S POLISHING PASTE. Used by Her Majesty...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_table('../data/subset/ads/ads.csv', header=None, nrows=5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inferring:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputFile=../data/subset/ads/ads.csv\n",
      "OutputDir=./model_ads\n",
      "Process=infer\n",
      "AllDir=./model_train\n",
      "Inferencer=./model_train/inferencer.model\n",
      "CORES=6\n",
      "SEED1=1\n",
      "SEED2=1\n",
      "TOPICS=250\n",
      "ITERATION=3000\n",
      "INTERVAL=40\n",
      "BURNIN=300\n",
      "IDFMIN=1\n",
      "IDFMAX=10\n",
      "04:09:22 :: Start import dataset...\n",
      " Rewriting extended pipe from ./model_train/import.model\n",
      "  Instance ID = 73e48a25-531d-4bd9-8b7e-acbfdaced195\n",
      "Import new data for inferring.\n",
      "04:10:01 :: Imported.\n",
      "04:10:01 :: Start prune model...\n",
      "04:10:14 :: Pruned.\n",
      "04:10:14 :: Start infering dataset...\n",
      "04:12:08 :: Inferred.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training portion = 1.0\n",
      "Validation portion = 0.0\n",
      "Testing portion = 0.0\n",
      "Prune info gain = 0\n",
      "Prune count = 0\n",
      "Prune df = 0\n",
      "idf range = 1.0-10.0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "features: 1414645 -> 171826\n",
      "Writing instance list to ./model_ads/pruned.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 8 ms, total: 24 ms\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%capture capt\n",
    "%%time\n",
    "%%bash\n",
    "#! /bin/bash\n",
    "\n",
    "bash ./model.sh -i '../data/subset/ads/ads.csv' -o './model_ads' -p 'infer';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write training log to file. This way to avoid MALLET print very long log in notebook.\n",
    "with open('./model_ads/ads.log', 'w') as f:\n",
    "    f.write(capt.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
